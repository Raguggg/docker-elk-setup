input {
  gelf {
    host => "0.0.0.0"
    port => 12201
  }
}

filter {
  # Add tags based on container names
  if [container_name] =~ "db" {
    mutate {
      add_tag => ["database_logs"]
    }
  } else if [container_name] =~ "web" {
    mutate {
      add_tag => ["web_logs"]
    }
  } else if [container_name] =~ "nginx" {
    mutate {
      add_tag => ["nginx_logs"]
    }
  } else {
    mutate {
      add_tag => ["unknown"]
    }
  }

  # Parse Nginx logs
  if "nginx_logs" in [tags] {
    grok {
      match => {
        "message" => '%{IP:client_ip} - - \[%{HTTPDATE:timestamp}\] "%{DATA:domain}" "%{WORD:http_method} %{URIPATH:route_endpoint}(?:%{URIPARAM:query_string})? HTTP/%{NUMBER:http_version}" %{NUMBER:status_code} %{NUMBER:body_bytes_sent} "(?:%{URI:referrer}|-)" "%{DATA:user_agent}"'
      }
    }

    # Convert the timestamp to a proper date format
    date {
      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
      target => "@timestamp"
    }

    # Remove the original timestamp field
    mutate {
      remove_field => ["timestamp"]
    }
  }

}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "logs-%{+yyyy.MM.dd}"  # Use a static prefix like "logs"
    data_stream => "auto"
    ssl => false  # Set to true if using SSL
    ssl_certificate_verification => false  # Disable SSL certificate verification if not using SSL
    
    # Ensure correct user/password for Elasticsearch security (environment variables can be used)
    user => "elastic"
    password => "x0lFM0AHFumZR07_a0ft"
  }

  # Debug output to console for verifying logs
  stdout {
    codec => rubydebug
  }
}